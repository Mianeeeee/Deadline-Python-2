\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{indentfirst}
\usepackage{xcolor}
\usepackage{minted}

\geometry{margin=1in}
\renewcommand{\thesubsection}{\Alph{subsection}}

\title{Report}
\author{Vu Viet Thai – B23DCCE085}
\date{}

\usemintedstyle{monokai}
\setminted{
    bgcolor=black,
    fontsize=\footnotesize,
    breaklines=true,
    framesep=2mm,
    baselinestretch=1.2,
    tabsize=4
}

\begin{document}

\maketitle
\pagestyle{plain}

\section{Problem\_1 - Multilayer Perceptron (MLP) Implementation}

\subsection{Introduction}

Problem\_1 presents the development and evaluation of a Multilayer Perceptron 
(MLP) model to solve image classification tasks on the CIFAR-10 dataset. 
The model is implemented using the PyTorch library, one of the most popular 
frameworks for machine learning and deep learning.\\

The CIFAR-10 dataset consists of 60,000 color images with dimensions of 32x32 
pixels, divided into 10 classes with 6,000 images per class. The classes 
include: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and 
truck.

\subsection{Methodology}

\subsubsection{Data Preparation}

First, data is prepared using transformations to normalize the images:

\begin{minted}{python3}
    transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
\end{minted}

\begin{itemize}
    \item ToTensor() converts PIL images or numpy arrays to PyTorch tensors
    \item Normalize() normalizes pixel values to the range [-1, 1] with mean=0.5 
    and std=0.5
\end{itemize}

Data is downloaded and split into training and testing sets:

\begin{minted}{python3}
    trainset = torchvision.datasets.CIFAR10(root='./SourceCode/data', train=True, download=True, transform=transform)
    trainloader = DataLoader(trainset, batch_size=64, shuffle=True)

    testset = torchvision.datasets.CIFAR10(root='./SourceCode/data', train=False, download=True, transform=transform)
    testloader = DataLoader(testset, batch_size=64, shuffle=False)
\end{minted}

\subsubsection{Model Architecture}

A 3-layer MLP is constructed with the following architecture:

\begin{minted}{python3}
    class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(32 * 32 * 3, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )
\end{minted}

\begin{itemize}
    \item Input layer: 32x32x3 = 3072 neurons (corresponding to input image size)
    \item First hidden layer: 512 neurons with ReLU activation function
    \item Second hidden layer: 256 neurons with ReLU activation function
    \item Output layer: 10 neurons (corresponding to 10 CIFAR-10 classes)
\end{itemize}

\subsubsection{Model Training}

The training process is performed with the following parameters:

\begin{minted}{python3}
    net = MLP().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(net.parameters(), lr=0.001)
\end{minted}

\begin{itemize}
    \item Loss function: CrossEntropyLoss (suitable for multi-class classification)
    \item Optimizer: Adam with learning rate 0.001
    \item Number of epochs: 6
    \item Batch size: 64
\end{itemize}

The training process is implemented as follows:

\begin{minted}{python3}
    for epoch in range(6):
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data[0].to(device), data[1].to(device)
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        if i % 100 == 99:
            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0
\end{minted}

\subsection{Results and Evaluation}

After 6 epochs of training, the model achieves accuracy on the test set as follows:

\begin{minted}{python3}
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f'Accuracy on test set: {100 * correct / total:.2f}%')
\end{minted}

The specific results depend on the run, but typically achieve around 45-55\% on the 
test set. This is a reasonable result for a simple MLP model on a complex dataset 
like CIFAR-10.

\section{Problem\_2 - Convolutional Neural Network (CNN) Implementation}

\subsection{Introduction}

Problem\_2 presents the development and evaluation of a Convolutional Neural Network 
(CNN) model to solve image classification tasks on the CIFAR-10 dataset. Compared to 
the MLP model implemented previously, CNN demonstrates many superior advantages in 
image data processing thanks to its ability to extract spatial features.

\subsection{Model Architecture}

The CNN model is implemented with 2 main components:

\subsubsection{Convolutional Part}

\begin{minted}{python3}
    self.conv_layers = nn.Sequential(
    nn.Conv2d(3, 32, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2, 2),  # 32x32 → 16x16
    nn.Conv2d(32, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2, 2),  # 16x16 → 8x8
    nn.Conv2d(64, 128, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2, 2)   # 8x8 → 4x4
)
\end{minted}

\begin{itemize}
    \item \textbf{First Conv2d layer}: 32 filters of size 3x3, padding=1 to preserve dimensions
    \item \textbf{MaxPooling}: Reduces spatial dimensions by half after each block
    \item Increasing number of filters through layers (32→64→128) to learn features 
    at multiple levels of abstraction
\end{itemize}

\subsubsection{Fully-Connected Part}

\begin{minted}{python3}
    self.fc_layers = nn.Sequential(
    nn.Flatten(),
    nn.Linear(128 * 4 * 4, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)
\end{minted}

\begin{itemize}
    \item Converts from 4D tensor to 2D using Flatten()
    \item Two fully-connected layers with ReLU activation function
    \item Output layer has 10 neurons corresponding to 10 CIFAR-10 classes
\end{itemize}

\subsection{Model Training}

The training process is performed with the following parameters:

\begin{minted}{python3}
    model = CNN().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
\end{minted}

\begin{itemize}
    \item \textbf{Loss function}: CrossEntropyLoss
    \item \textbf{Optimizer}: Adam with learning rate 0.001
    \item \textbf{Number of epochs}: 9
    \item \textbf{Batch size}: 64 (inherited from DataLoader)
\end{itemize}

Training process:

\begin{minted}{python3}
    for epoch in range(9):
    running_loss = 0.0
    for i, (inputs, labels) in enumerate(trainloader, 0):
        # Forward propagation
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        # Backward propagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Track loss
        running_loss += loss.item()
        if i % 100 == 99:
            print(f'[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 100:.3f}')
            running_loss = 0.0
\end{minted}

\subsection{Results and Evaluation}

The model achieves approximately \textbf{70-75\%} accuracy on the test set, a significant 
improvement over MLP (45-55\%).

\begin{minted}{python3}
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f'Accuracy on test set: {100 * correct / total:.2f}%')
\end{minted}

\section{Problem\_3 - Comparative Analysis with Enhanced Evaluation}

\subsection{Introduction}

Problem\_3 presents a comparison of performance between two neural network 
architectures: Multilayer Perceptron (MLP) and Convolutional Neural Network (CNN) 
on the CIFAR-10 dataset. Particularly, we have improved the evaluation process by 
splitting the training set into train (80\%) and validation (20\%) to monitor the 
learning process more rigorously.

\subsection{Data Preparation}

Data is prepared with the following steps:

\begin{minted}{python3}
    # Load entire trainset
    full_trainset = torchvision.datasets.CIFAR10(root='./SourceCode/data', train=True, download=True, transform=transform)

    # Split into train and validation sets
    train_size = int(0.8 * len(full_trainset))
    val_size = len(full_trainset) - train_size
    trainset, valset = random_split(full_trainset, [train_size, val_size])

    # Create DataLoader for validation
    valloader = DataLoader(valset, batch_size=64, shuffle=False)
\end{minted}

\subsection{Model Architecture}

\subsubsection{MLP Model}

\begin{minted}{python3}
    class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(32 * 32 * 3, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )
\end{minted}

\subsubsection{CNN Model}

\begin{minted}{python3}
    class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__():
        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            # ... (2 similar convolutional blocks)
        )
        self.fc_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128*4*4, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )
\end{minted}

\subsection{Training Process}

Implementing a unified train\_model function for both architectures:

\begin{minted}{python3}
    def train_model(model, trainloader, valloader, num_epochs=10):
    # Initialize optimizer and loss function
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # Track metrics
    train_losses = []
    train_accuracies = []
    val_accuracies = []
    
    for epoch in range(num_epochs):
        model.train()
        # Training process
        for inputs, labels in trainloader:
            # Forward propagation, backward propagation, and weight updates
            ...
        
        # Evaluate on validation set
        val_acc = evaluate_model(model, valloader)
        
        # Save metrics
        train_losses.append(avg_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)
\end{minted}

\subsection{Results and Evaluation}

Problem 3 has demonstrated the clear superiority of CNN over MLP in image 
classification tasks. Under the same training conditions, CNN achieves ~25\% higher 
performance on the test set. This confirms the importance of selecting appropriate 
architecture for computer vision problems.

\section{Problem\_4 - Learning Curve Visualization and Analysis}

\subsection{Introduction}

The development of machine learning models, especially neural networks, requires 
careful monitoring of training dynamics to ensure optimal performance. Learning 
curves, plotting training loss and accuracy along with validation accuracy across 
epochs, are important tools for evaluating model convergence, overfitting phenomena, 
and generalization capability. Problem\_4 focuses on the training process of MLP 
and CNN models, using data extracted from Problem\_3 and visualizing it in two sets 
of charts. The source code uses Matplotlib to create these curves, providing deep 
insights into the learning behavior of the models.

\subsection{Methodology}

Problem\_4 uses the Matplotlib library to create learning curves for both MLP and 
CNN models. The save\_history() function from the Problem\_3 module retrieves 
training history data, including training loss (train\_loss), training accuracy 
(train\_acc), and validation accuracy (val\_acc) for both models. Below is the main 
code from the script:

\begin{minted}{python3}
    mlp_train_loss, mlp_train_acc, mlp_val_acc, cnn_train_loss, cnn_train_acc, cnn_val_acc = save_history()
\end{minted}

The plot\_learning\_curves() function is defined to plot two subplots on one figure: 
one chart for training loss and one chart for training and validation accuracy. 
Epochs are represented on the x-axis, while loss and accuracy are represented on 
the y-axis. Colors and line styles are used to distinguish metrics: solid red line 
(r-) for training loss, solid blue line (b-) for training accuracy, and dashed 
green line (g--) for validation accuracy. Below is the important code:

\begin{minted}{python3}
    def plot_learning_curves(train_loss, train_acc, val_acc, title='Model'):
    epochs = range(1, len(train_loss) + 1)
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_loss, 'r-', label='Train Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title(f'{title} - Training Loss')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_acc, 'b-', label='Train Acc')
    plt.plot(epochs, val_acc, 'g--', label='Val Acc')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.title(f'{title} - Accuracy')
    plt.legend()
    
    plt.tight_layout()
    filename = f'SourceCode/Draw-the-learning-curve/{title}.png'
    plt.savefig(filename)
    print(f"Successfully saved the {title} learning model chart!")
    plt.show()
    plt.close()
\end{minted}

This function is called twice, once each for MLP and CNN, with corresponding titles:

\begin{minted}{python3}
    plot_learning_curves(mlp_train_loss, mlp_train_acc, mlp_val_acc, title='MLP')
    plot_learning_curves(cnn_train_loss, cnn_train_acc, cnn_val_acc, title='CNN')
\end{minted}

The charts are saved as PNG files and displayed on screen, allowing for visual 
analysis.

\subsection{Results and Evaluation}

\subsubsection{MLP Learning Curves}

The training loss chart for MLP shows values starting from around 1.6 and gradually 
decreasing to around 1.0 after 10 epochs, reflecting improvement during the training 
process. The accuracy chart shows training accuracy (blue) increasing from around 
45\% to 70\%, while validation accuracy (green) increases from 50\% to around 75\%. 
The small difference between these two curves indicates the model is not 
significantly overfitting, although there are potential signs of suboptimal 
generalization.

\subsubsection{CNN Learning Curves}

For CNN, training loss starts from 1.4 and decreases to around 0.2 after 10 epochs, 
showing better convergence compared to MLP. The accuracy chart shows training 
accuracy (blue) increasing from 50\% to nearly 90\%, while validation accuracy 
(green) increases from 60\% to around 90\%. The two accuracy curves are close 
together, indicating CNN has better generalization capability than MLP, with less 
overfitting.

\subsubsection{Evaluation and Conclusion}

The results show CNN outperforms MLP in both loss and accuracy, especially in 
generalization. This may be due to the characteristics of the dataset, likely image 
data, where CNN demonstrates its advantages. For further optimization, the following 
steps are recommended:

\begin{itemize}
    \item Increase the number of training epochs to check saturation points
    \item Use regularization techniques like dropout to reduce overfitting risk
    \item Adjust hyperparameters and use methods like cross-validation for better 
    model evaluation
\end{itemize}

\section{Problem\_5 - Confusion Matrix Analysis}

\subsection{Introduction}

Problem\_5 analyzes the performance of two models, MLP and CNN, on the CIFAR-10 
dataset, which consists of 10 classes (airplane, automobile, bird, cat, deer, dog, 
frog, horse, ship, truck) with 60,000 32x32 color images. Confusion matrices are 
created to evaluate the accuracy and classification errors of each model. Problem\_5 
uses libraries such as PyTorch, Matplotlib, and Seaborn to visualize results, with 
data obtained from a test set through the testloader function.

\subsection{Methodology}

Problem\_5 uses PyTorch to define and evaluate two models, MLP and CNN, along with 
Matplotlib and Seaborn libraries to plot confusion matrices. Below are the main 
steps in the process:

\subsubsection{Model Initialization}

Two models MLP and CNN are declared from the Problem\_3 module:

\begin{minted}{python3}
    mlp_model = MLP()
    cnn_model = CNN()
\end{minted}

\subsubsection{Creating Confusion Matrix}

The plot\_confusion\_matrix function is defined to calculate and visualize the 
confusion matrix. The model is switched to evaluation mode (model.eval()) to avoid 
gradient updates. Predictions and actual labels are collected from the test set:

\begin{minted}{python3}
    def plot_confusion_matrix(model, dataloader, classes, title='Model'):
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    cm = confusion_matrix(all_labels, all_preds)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=classes, yticklabels=classes)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.tight_layout()
    plt.savefig(f"SourceCode/Confusion-matrix/{title}.png")
    plt.show()
    plt.close()
\end{minted}

\subsubsection{Execution and Visualization}

The function is called twice for each model with the CIFAR-10 class list:

\begin{minted}{python3}
    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 
          'dog', 'frog', 'horse', 'ship', 'truck']

    plot_confusion_matrix(mlp_model, testloader, classes, title='MLP')
    plot_confusion_matrix(cnn_model, testloader, classes, title='CNN')
\end{minted}

The confusion matrix is plotted as a heatmap with numerical values (annot=True) and 
blue color scale (cmap='Blues'), where the x-axis represents predicted labels and 
y-axis represents true labels.

\subsection{Results and Evaluation}

\subsubsection{MLP Confusion Matrix}

The MLP confusion matrix shows detailed classification performance for each class. 
Some key observations:

\begin{itemize}
    \item The "airplane" class (23 actual samples) was incorrectly predicted as 
    "frog" (4 samples), "horse" (25 samples), and "truck" (19 samples), with a 
    total of 164 incorrect predictions.
    \item The "automobile" class (67 actual samples) had 21 samples confused with 
    "frog" and 173 samples with "truck".
    \item The "cat" class (52 samples) had 33 samples confused with "frog" and 73 
    samples with "dog".
    \item The "dog" class (54 samples) had 48 samples correctly predicted, but 74 
    samples confused with "frog".
    \item The total correct predictions (values on the main diagonal) are 764 for 
    "airplane", 687 for "automobile", etc., showing MLP has an overall accuracy of 
    around 70-75\% based on previous learning curves.
\end{itemize}

\subsubsection{CNN Confusion Matrix}

The CNN confusion matrix demonstrates superior performance:

\begin{itemize}
    \item The "airplane" class (405 actual samples) had 115 samples confused with 
    "automobile" and 480 samples with "ship", but the total correct predictions are 
    very high (405).
    \item The "automobile" class (351 samples) had 120 samples confused with 
    "truck" and 529 correct samples.
    \item The "cat" class (385 samples) had 123 samples confused with "dog" and 491 
    correct samples.
    \item The "dog" class (384 samples) had 88 samples confused with "cat" and 527 
    correct samples.
    \item The total correct predictions on the main diagonal show CNN achieves 
    nearly 90\% accuracy, consistent with previous learning curves.
\end{itemize}

\subsubsection{Analysis}

Comparing the two matrices, CNN clearly outperforms MLP in overall accuracy. The 
number of correct predictions on CNN's main diagonal (e.g., 405 for "airplane", 529 
for "automobile") is significantly higher than MLP (764 and 687 respectively, but 
on a smaller total sample size). This shows CNN has better ability to distinguish 
between classes, especially with image data, thanks to convolutional layers that 
help extract spatial features.\\

MLP tends to have more confusion between similar classes (like "cat" and "dog" or 
"airplane" and "truck"), due to limitations in processing 2D image data without 
convolutional mechanisms. Conversely, CNN minimizes these errors, although there 
are still some notable confusions (like "airplane" with "ship" or "automobile" with 
"truck"), possibly due to similar shapes.\\

A noteworthy point is that MLP incorrectly predicts more samples into classes like 
"frog" and "truck", while CNN distributes errors more evenly and focuses on 
accurate classes. This reinforces that CNN is more suitable for the CIFAR-10 
dataset, which contains complex images.

\subsubsection{Conclusion}

Analysis of the confusion matrices for MLP and CNN on the CIFAR-10 dataset shows 
CNN has superior performance with nearly 90\% accuracy, while MLP achieves around 
70-75\%. This result reflects CNN's advantage in image processing thanks to its 
ability to extract spatial features. Improvement suggestions include data 
augmentation and model fine-tuning to minimize classification errors.\\\\

\textbf{\Large{Final Summary}}

This comprehensive study demonstrates the clear superiority of Convolutional Neural 
Networks (CNN) over Multilayer Perceptrons (MLP) for image classification tasks on 
the CIFAR-10 dataset. Through systematic experimentation and analysis across five 
problems, we have established that:

\begin{itemize}
    \item \textbf{Performance Gap}: CNN consistently outperforms MLP by approximately 
    20-25\% in classification accuracy
    \item \textbf{Learning Dynamics}: CNN shows better convergence properties and 
    generalization capabilities
    \item \textbf{Error Analysis}: CNN makes fewer classification errors and shows 
    more balanced confusion patterns
    \item \textbf{Architectural Advantages}: The convolutional layers' ability to 
    extract spatial features makes CNN inherently more suitable for computer vision 
    tasks
\end{itemize}

The implementation using PyTorch framework provides a solid foundation for 
understanding deep learning principles and their practical applications in image 
classification. The methodical approach from basic MLP implementation to advanced 
CNN architecture, enhanced with proper validation techniques and comprehensive 
visualization, offers valuable insights for machine learning practitioners and 
researchers.\\

Future work could explore more advanced CNN architectures, data augmentation 
techniques, and regularization methods to further improve performance on the 
CIFAR-10 classification task.

\end{document}